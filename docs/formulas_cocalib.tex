\documentclass[10pt]{article}

% packages
\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{import}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}
\usepackage{autobreak}

% paragraph spacing
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% define vector and matrix representations
%\renewcommand{\vec}[1]{\textbf{#1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
%\renewcommand{\vec}[1]{\uppercase{#1}}
\newcommand{\mat}[1]{\boldsymbol{#1}}

% set graphics path
\graphicspath{{images/}}

% Authors and Affiliations
\title{Co-Calibration with Bayesian Inference to Update Linear Affine Model}
\author{Maximilian Gruber}    % maximilian.gruber@ptb.de
\date{November 2021}
    
\begin{document}
\maketitle

It is of interest to calibrate a sensor in a way that complies with the VIM \cite{bipm_2012}. 
This involves to:

\begin{quote}[VIM 2.39]
    establish[\dots] a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties
\end{quote}

\begin{quote}[VIM 2.39]
    use[\dots] this information to establish a relation for obtaining a measurement result from an indication
\end{quote}

It is therefore necessary to mathematically describe the sensor transfer behavior, measurement data and regression task to estimate the former from the latter.
The effect of simplifications of the proposed assumptions need to be investigated.


\section{Sensor Model}
We consider sensors with an ideal linear affine measurement function from reference $x$ to indication $y$.
This adopts the nomenclature and notation of \cite{bipm_2008,bipm_2012}.
\begin{align}
    y(t) &= a \cdot x(t) + b \nonumber \\
       &= f(x(t), \vec{\theta}) \\
    \vec{\theta} &= \begin{bmatrix} a \\ b \end{bmatrix}
\end{align}


\section{Data Model}
The assumed data model is detailed in \cite{dellaportas_1995}.
The reference device does not provide the actual (true) value of the measurand $X_{ai}$ but an observed value $X_{oi}$ that differs by an (unknown) error term $\tilde{\varepsilon}_i$.
Knowledge about the distribution of $\tilde{\varepsilon}_i$ can be derived from the uncertainty $u_x(t)$ of the reference device.
The indication $Y_i$ of the DUT differs from the ideal model $f(X_{ai}, \vec{\theta})$ by an error $\varepsilon_i$.
The distribution of $\varepsilon_i$ is not fully known and needs to be identified as part of the calibration process.
The statistical model therefore looks like \cite{dellaportas_1995}:
\begin{align}
    Y_i &= f(X_{ai}, \vec{\theta}) + \varepsilon_i \\
    X_{oi} &= X_{ai} + \tilde{\varepsilon}_i
\end{align}

With the following assumptions on the distributions of the errors
\begin{align}
    \varepsilon_i &\propto \mathcal{N}(\vec{0}, \sigma_y^2 \cdot I_n)\\
    \tilde{\varepsilon}_i &\propto \mathcal{N}(\vec{0}, \mat{U_x}) 
\end{align}
\begin{align}
    \mat{U_x} &= \begin{bmatrix} u^2(t_k) && \\ &\ddots& \\ && u^2(t_{k+n-1}) \end{bmatrix} 
\end{align}

Available datapoints take the form of $\vec{\delta}_i$ and the proposed method operates on a set $\Delta$ containing $n$ datapoints $\vec{\delta}_i$ at a time.
\begin{align}
    \vec{\delta}_i &= [t_i, y(t_i), x(t_i), u_x(t_i)]^T \nonumber \\
                 &= [t_i, Y_i, X_{oi}, \sigma_{xi})]^T \\
    \Delta &= \{\vec{\delta}_k, \vec{\delta}_{k+1}, \dots,  \vec{\delta}_{k+n-1}\} 
\end{align}


\section{Model Identification as by Dellaportas}
The joint distribution of the mentioned variables is
\begin{align}
    p(\vec{\theta}, \vec{X_a}, \sigma_y, \underbrace{\vec{Y}, \vec{X_o}, \sigma_x}_{\Delta})
\end{align}

The parameters $\vec{\theta}$, the actual reference values $X_{ai}$ and $\sigma_{y}$ are unknown.
The main interest lies in inferring knowledge about these unknowns from the measurement data $\Delta$ via Bayesian inference.
\begin{align}
    \underbrace{p(\vec{\theta}, \vec{X_a} | \vec{Y}, \vec{X_o})}_{\text{posterior}} \propto \underbrace{p(\vec{Y} | \vec{X_a}, \vec{\theta}, \vec{X_o})}_{\text{likelihood}} \underbrace{p(\vec{\theta}, \vec{X_a} | \vec{X_o})}_{\text{prior}}
\end{align}

This leads to the following conditionals of a Gibbs Sampler \cite{dellaportas_1995}:
\begin{align}
    p(\vec{X_a} | \vec{\theta}, \sigma_y, \sigma_x, \vec{Y}, \vec{X_o}) &\propto {\color{cyan} p(\vec{Y} | \vec{X_a}, \vec{\theta}, \sigma_y)} {\color{red} p(\vec{X_a} | \sigma_x, \vec{X_o})} \label{eq:posterior_X_a}\\
    p(\theta_j | \theta_{(j)}, \vec{X_a}, \sigma_y, \vec{Y}) &\propto {\color{cyan} p(\vec{Y} | \vec{X_a}, \vec{\theta}, \sigma_y)} \underbrace{p(\theta_j | \theta_{(j)}, \vec{X_a}, \sigma_{y})}_{\text{assume } p(\theta_j)} \label{eq:posterior_params} \\
    p(\sigma_y | \vec{\theta}, \vec{X_a}, \vec{Y}) &\propto {\color{cyan} p(\vec{Y} | \vec{X_a}, \vec{\theta}, \sigma_y)} \underbrace{p(\sigma_y | \vec{\theta}, \vec{X_a})}_{\text{assume } p(\sigma_y)} \label{eq:posterior_sigma_y}
\end{align}


\section{Likelihoods}
\begin{align}
    {\color{cyan} p(\vec{Y} | \vec{X_a}, \vec{\theta}, \sigma_y)} &\propto \frac{1}{\sigma_y^n} \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N \left( Y_i - f(X_{ai}, \vec{\theta}) \right)^2 \right\} \\
    {\color{red} p(\vec{X_a} | \sigma_x, \vec{X_o})} &\propto \frac{1}{\sqrt{|\mat{U_x}|}} \exp\left\{ - \frac{1}{2} (\vec{X_a} - \vec{X_o})^T \mat{U_x}^{-1} (\vec{X_a} - \vec{X_o}) \right\}
\end{align}    

\section{Posteriors}
\subsection{Posterior for $\vec{X_a}$}
Evaluate \cref{eq:posterior_X_a}.
\begin{align}
    p(\vec{X_a} | \vec{\theta}, \sigma_y, \sigma_x, \vec{Y}, \vec{X_o}) &\propto {\color{cyan} p(\vec{Y} | \vec{X_a}, \vec{\theta}, \sigma_y)} {\color{red} p(\vec{X_a} | \sigma_x, \vec{X_o})} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N \left( Y_i - a X_{ai} - b \right)^2 \right\} \cdot \exp\left\{ - \frac{1}{2} (\vec{X_a} - \vec{X_o})^T \mat{U_x}^{-1} (\vec{X_a} - \vec{X_o}) \right\} \\
    
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N (a^2 X_{ai}^2  + 2 a b X_{ai} - 2 a Y_i X_{ai} + Y_i^2 + b^2 -2 b Y_i) \right\} \nonumber \\
    &\quad \cdot \exp\left\{ - \frac{1}{2} ( \vec{X_a}^T \mat{U_x}^{-1} \vec{X_a} - \vec{X_a}^T \mat{U_x}^{-1} \vec{X_o} -\vec{X_o}^T \mat{U_x}^{-1} \vec{X_a} + \vec{X_o}^T \mat{U_x}^{-1} \vec{X_o} ) \right\} \\
    
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N (a^2 X_{ai}^2  + 2 a (b - Y_i) X_{ai} ) \right\} \nonumber \\
    &\quad \cdot \exp\left\{ - \frac{1}{2} ( \vec{X_a}^T \mat{U_x}^{-1} \vec{X_a} - \vec{X_a}^T \mat{U_x}^{-1} \vec{X_o} -\vec{X_o}^T \mat{U_x}^{-1} \vec{X_a}) \right\} \\
    
    &\propto \exp\left\{ - \frac{1}{2} (\vec{X_a}^T \mat{F_1} \vec{X_a} + \vec{X_a}^T \vec{F_2} + \vec{F_2}^T \vec{X_a}) \right\} \nonumber \\
    &\quad \cdot \exp\left\{ - \frac{1}{2} ( \vec{X_a}^T \mat{U_x}^{-1} \vec{X_a} - \vec{X_a}^T \mat{U_x}^{-1} \vec{X_o} -\vec{X_o}^T \mat{U_x}^{-1} \vec{X_a}) \right\} \\
    
    &\propto \exp\left\{ - \frac{1}{2} (\vec{X_a}^T \underbrace{(\mat{F_1} + \mat{U_x}^{-1})}_{\mat{V}^{-1}} \vec{X_a} - \vec{X_a}^T (- \vec{F_2} + \mat{U_x}^{-1} \vec{X_o}) - (- \vec{F_2}^T + \vec{X_o}^T \mat{U_x}^{-1}) \vec{X_a}) \right\} \\
    
    &\propto \exp\left\{ - \frac{1}{2} (\vec{X_a}^T \mat{V}^{-1} \vec{X_a} - \vec{X_a}^T \mat{V}^{-1} \underbrace{\mat{V} (\mat{U_x}^{-1} \vec{X_o} - \vec{F_2}) }_{\vec{M}} - \vec{M}^T \mat{V}^{-1} \vec{X_a}) \right\} \\
    
    &\propto \exp\left\{ - \frac{1}{2} (\vec{X_a} - \vec{M})^T \mat{V}^{-1} (\vec{X_a} - \vec{M}) \right\}

\end{align}
Which corresponds to a multivariate Gaussian $\mathcal{N}(\vec{M}, \mat{V})$. 
The following matrices are introduced to transform the sum into a matrix operation:
\begin{align}
    \mat{F_1} &= \frac{a^2}{\sigma_y^2} \cdot \mat{I_N} \\
    \vec{F_2} &= \frac{a}{\sigma_y^2} \begin{bmatrix} b-Y_1 & \dots & b-Y_N \end{bmatrix}^T
\end{align}

\subsection{Posterior for $a$}
Evaluate \cref{eq:posterior_params} for $\theta_i= a$.
\begin{align}
    p(a|b, \vec{X_a}, \sigma_y, \vec{Y}) &\propto {\color{cyan} p(\vec{Y} | \vec{X_a}, \vec{\theta}, \sigma_y)} \underbrace{p(a | b, \vec{X_a}, \sigma_{y})}_{\mathcal{N}(\mu_a, \sigma_a^2)} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N \left( Y_i - a X_{ai} - b \right)^2 \right\} \cdot \exp\left\{ - \frac{1}{2\sigma_a^2} ( a - \mu_a )^2 \right\} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N (a^2 X_{ai}^2  + 2 a b X_{ai} - 2 a Y_i X_{ai} + Y_i^2 + b^2 -2 b Y_i) - \frac{1}{2\sigma_a^2} ( a^2 - 2 a \mu_a - \mu_a^2 ) \right\} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N (a^2 X_{ai}^2  + 2 a (b - Y_i) X_{ai} ) - \frac{1}{2\sigma_a^2} ( a^2 - 2 a \mu_a ) \right\} \\
    &\propto \exp\left\{ - 2 a \underbrace{\left[ \sum_{i=1}^N \frac{(b - Y_i) X_{ai}}{2\sigma_y^2} - \frac{\mu_a}{2\sigma_a^2} \right]}_{B} + a^2 \underbrace{\left[ - \sum_{i=1}^N \frac{X_{ai}^2}{2\sigma_y^2}  - \frac{1}{2\sigma_a^2} \right]}_{A} \right\} \\
    &\propto \exp\left\{ A (a - \frac{B}{A})^2\right\}
\end{align}
Which corresponds to a Gaussian $\mathcal{N}(\frac{B}{A}, -\frac{1}{2A})$


\subsection{Posterior for $b$}
Evaluate \cref{eq:posterior_params} for $\theta_i= b$.
\begin{align}
    p(b|a, \vec{X_a}, \sigma_y, \vec{Y}) &\propto {\color{cyan} p(\vec{Y} | \vec{X_a}, \vec{\theta}, \sigma_y)} \underbrace{p(b | a, \vec{X_a}, \sigma_{y})}_{\mathcal{N}(\mu_b, \sigma_b^2)} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N \left( Y_i - a X_{ai} - b \right)^2 \right\} \cdot \exp\left\{ - \frac{1}{2\sigma_b^2} ( b - \mu_b )^2 \right\} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N (a^2 X_{ai}^2  + 2 a b X_{ai} - 2 a Y_i X_{ai} + Y_i^2 + b^2 -2 b Y_i )- \frac{1}{2\sigma_b^2} ( b^2 - 2 b \mu_b - \mu_b^2 ) \right\} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \sum_{i=1}^N (2 b (a X_{ai} - Y_i) + b^2 ) - \frac{1}{2\sigma_b^2} ( b^2 - 2 b \mu_b ) \right\} \\
    &\propto \exp\left\{ - 2 b \underbrace{\left[ \sum_{i=1}^N \frac{a X_{ai} - Y_i}{2\sigma_y^2} - \frac{\mu_b}{2\sigma_b^2} \right]}_{B} + b^2 \underbrace{\left[ - \frac{N}{2\sigma_y^2}  - \frac{1}{2\sigma_b^2} \right]}_{A} \right\} \\
    &\propto \exp\left\{A (a - \frac{B}{A})^2\right\}
\end{align}
Which corresponds to a Gaussian $\mathcal{N}(\frac{B}{A}, -\frac{1}{2A})$


\subsection{Posterior for $\sigma_y$}
Evaluate \cref{eq:posterior_sigma_y}.
\begin{align}
    p(\sigma_y | \vec{\theta}, \vec{X_a}, \vec{Y}) &\propto {\color{cyan} p(\vec{Y} | \vec{X_a}, \vec{\theta}, \sigma_y)} \underbrace{p(\sigma_y | \vec{\theta}, \vec{X_a})}_{\mathcal{N}(\mu_{\sigma}, \sigma_{\sigma}^2)} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \underbrace{\sum_{i=1}^N \left( Y_i - a X_{ai} - b \right)^2}_{\tilde{A}} \right\} \cdot \exp\left\{ - \frac{1}{2\sigma_{\sigma}^2} ( \sigma_y - \mu_{\sigma} )^2 \right\} \\
    &\propto \exp\left\{ - \frac{1}{2\sigma_y^2} \tilde{A} - \frac{1}{2\sigma_{\sigma}^2} ( \sigma_y^2 - \sigma_y \mu_{\sigma} + \mu_{\sigma}^2 ) \right\} \\
    &\propto \exp\left\{ \sigma_y^2 \left( - \frac{1}{2\sigma_{\sigma}^2} \right) + \sigma_y \left( -  \frac{\mu_{\sigma}}{2\sigma_{\sigma}^2}\right) + \frac{1}{\sigma_y^2} \left( - \frac{\tilde{A}}{2} \right) \right\} 
\end{align}

    
\section{Simplifications}

\subsection{Deterministic Reference}
$\vec{X_a} = \vec{X_o}$

\subsection{Known Variance}
$\sigma_y \equiv \text{known constant}$

\subsection{Both?}


\section{Method Outline}
\begin{enumerate}
    \item obtain new data $\Delta$
    \item obtain posteriors, if necessary approximate result as gaussian
    \item check if requirements on calibration accuracy are fulfilled
    \item terminate co-calibration or repeat with further data
\end{enumerate}

\bibliographystyle{plain}
\bibliography{references.bib}
    
\end{document}